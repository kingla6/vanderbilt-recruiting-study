---
title: "00-clean-recruit-data"
output:
  html_document:
    code_folding: hide
    toc_float: TRUE
---

```{r}
knitr::opts_chunk$set(message = FALSE)
```

# Overview

This notebook combines recruit data collected from the cfbscrapR package (original recruit data sourced from 247sports.com) with county data for the hometown cities that recruits are from along with geolocation data for recruit cities and committed schools. The point of this county data being merged in is to aid in creating heap maps at the county level for future analysis. The point of this location data is to aid in distance analysis of recruits and schools.

*This code needs a valid Google API key to run*  
  
*Location, final merge, and export sections commented out for knitting purposes*

# Methodology 

Load relevant packages
```{r}
library(tidyverse)
library(janitor)
library(cfbscrapR)
library(assertr)
library(assertable)
library(ggmap)
library(geosphere)
```

## County Data

### Collection

Load data for `uscities` and `uszips` to be later joined with the recruit data. This will allow for creation of heat maps in future analysis.

* [uscities data](https://simplemaps.com/data/us-cities)
* [uszips data](https://simplemaps.com/data/us-zips)

Load data for `school_locs` to be later joined with recruit data. This will allow for distance analysis of recruits and schools. 

* Courtesy of Conor McQuiston, Twitter: @ConorMcQ5
 
```{r}
#saved csv files
setwd('C:/Users/kingl/Desktop/Projects/football/vanderbilt-recruiting-study/preliminary-data/')
uscities <- read.csv('uscities.csv') %>% clean_names()
uszips <- read.csv('uszips.csv') %>% clean_names()

#fileEcnoding takes care of the weird text added to the first column name
school_locs <- read.csv('edu_ranks.csv', fileEncoding="UTF-8-BOM") %>% clean_names()

#api key to be used later for recruit locations
register_google('YOUR API KEY HERE')
```

### Cleaning

Data cleaning of `uscities`
```{r}
#the goal is to remove duplicate cities from the uscities dataframe

#figure out which cities are duplicated
duplicate_cities <- uscities %>% 
  #group by state and city
  group_by(state_id, city_ascii) %>% 
  #find how many times each city, state appears
  summarise(count = n()) %>% 
  #display those which appear more than once
  filter(count>1) %>% 
  #ungroup
  ungroup() %>% 
  #left join data from uscities to the list of duplicates 
  #(want to keep only the observation of a given city with the largest population)
  left_join(uscities, by = c('state_id', 'city_ascii')) %>% 
  #group by state and city again
  group_by(state_id, city_ascii) %>% 
  #now find the max population observation for each of the duplicate cities
  summarise(max_pop = max(population)) %>% 
  #create unique identifier for city, state by combining the two
  mutate(city_st = paste0(city_ascii, state_id))

#create indicator column of whether or not observation should remain in dataset
uscities$keep = NA
#iterate over rows to determine if the city is part of the list of duplicated cities
#if the observation is on the list of duplicated cities and does not contain the max population, keep = 0
#all else, keep = 1
for(i in 1:nrow(uscities)){
  if(paste0(uscities$city_ascii[i], uscities$state_id[i]) %in% duplicate_cities$city_st){
    if(uscities$population[i] != 
       duplicate_cities$max_pop[duplicate_cities$city_st == paste0(uscities$city_ascii[i],
                                                                   uscities$state_id[i])]){
      uscities$keep[i] = 0
      } else{uscities$keep[i] = 1}
    } else{uscities$keep[i] = 1}
}

#filter uscities to only contain observations where keep == 1, update uscities
uscities <- uscities %>% 
  filter(keep == 1) %>% 
  select(-keep)
```

Data Cleaning of `uszips` (will be converted to `zip_cities`)
```{r}
#the goal is to remove duplicate cities 
#(we aren't necessarily worried about zip codes, the uszips dataframe contains cities not seen in the uscities dataframe, which helps with our analysis)

#begin with finding the max population of each city
max_pops <- uszips %>% 
  #group by state, city, county (since observations are individual zipcodes)
  group_by(state_id, city, county_fips, county_name) %>% 
  #find the sum of the population over each group
  summarize(population = sum(population)) %>% 
  #ungroup
  ungroup() %>% 
  #group by state city and county (grouped as such because some counties have multiple fips codes)
  group_by(state_id, city, county_name) %>% 
  #find the max population for each group 
  summarize(population = max(population))

#this chunk is to join fips codes back into observations gathered from max_pops
zip_cities <- uszips %>% 
  #group by state, city, county
  group_by(state_id, city, county_fips, county_name) %>% 
  #find max population for each group
  summarize(population = sum(population)) %>% 
  #ungroup
  ungroup() %>% 
  #inner join max_pops to subset to only those observations
  inner_join(max_pops,
             by = c('state_id', 'city', 'county_name', 'population'))

#now figure out which cities in zip_cities are duplicated
duplicate_zip_cities <- zip_cities %>% 
  #group by state, city
  group_by(state_id, city) %>% 
  #find how many times each group appears
  summarize(count = n()) %>% 
  #subset to those which appear multiple times
  filter(count > 1) %>% 
  #join  zip cities to combine county and population data
  left_join(zip_cities, by = c('state_id', 'city')) %>% 
  #group by state and city
  group_by(state_id, city) %>% 
  #find the max population for each group
  summarize(max_pop = max(population)) %>% 
  #create unique identifier for city, state by combining the two
  mutate(city_st = paste0(city, state_id))

#create indicator column of whether or not observation should remain in dataset
zip_cities$keep = NA
#iterate over rows to determine if the city is part of the list of duplicated cities
#if the observation is on the list of duplicated cities and does not contain the max population, keep = 0
#all else, keep = 1
for(i in 1:nrow(zip_cities)){
  if(paste0(zip_cities$city[i], zip_cities$state_id[i]) %in% duplicate_zip_cities$city_st){
    if(zip_cities$population[i] != 
       duplicate_zip_cities$max_pop[duplicate_zip_cities$city_st == paste0(zip_cities$city[i],
                                                                           zip_cities$state_id[i])]){
      zip_cities$keep[i] = 0
    } else{zip_cities$keep[i] = 1}
  } else{zip_cities$keep[i] = 1}
}

#filter zip_cities to only contain observations where keep == 1, update zip_cities
zip_cities <- zip_cities %>% 
  filter(keep == 1) %>% 
  select(-keep)
```

## Recruiting Data

### Collection

Pull recruiting player data from cfbscrapr
```{r}
#specify years
current_year <- Sys.Date() %>% format('%Y') %>% as.numeric()
years <- seq(2000, current_year)
#initialize empty dataframe for data to be stored in
recruiting_player <- NULL

#run function for each year, append to previously initialized dataframe
for(i in years) {
  recruiting_player = rbind(recruiting_player, cfb_recruiting_player(year = i))
}

#remove duplicate observations, provinces outside of the US, and countries other than US (US is NA)
recruiting_player <- recruiting_player %>% 
  unique() %>% 
  filter(country == 'USA',
         !state_province %in% c('QC', 'ON', 'AS', 'BC', 'SW', 'NR', 'QL', 'AB', 'EN',
                                'NA', '', 'GERM', 'AUST', 'CANA', 'FINL', 'ITAL', 'SWED',
                                'ENGL', 'BELG', 'DENM', 'FRAN', 'UK', 'AMER', 'NETH', 'VC', 'NEW'))

```

Remove duplicated player observations that are not perfect duplicates.
This will not catch every instance (player names not matching, etc.), but it is a good start.
```{r}
duplicates <- recruiting_player %>% 
  group_by(year, name, city, committed_to) %>% 
  summarize(count = n(),
            highest_ranking = min(ranking),
            lowest_ranking = max(ranking)) %>% 
  filter(count == 2,
         !is.na(committed_to)) %>% 
  select(-count, -highest_ranking)

recruiting_player <- recruiting_player %>% 
  anti_join(duplicates, by = c('year', 'name', 'city', 'committed_to', 'ranking' = 'lowest_ranking'))
```

### Cleaning

Clean city and state_province columns within recruiting data
```{r}
recruiting_player <- recruiting_player %>% 
  #convert na strings to NA for city and state
  mutate(city = ifelse(mgsub::mgsub(city, 
                                    pattern = c("[^[:alnum:] ]", " "), 
                                    replacement = '', 
                                    recycle = T) %>% tolower() == 'na',
                       NA, city),
         state_province = ifelse(mgsub::mgsub(state_province, 
                                              pattern = c("[^[:alnum:] ]", " "), 
                                              replacement = '', 
                                              recycle = T) %>% tolower() == 'na',
                                 NA, state_province)
         #below alterations to city were to account for cities which were present in recruiting data, but absent in uscities, adding these cities in did not substantially improve upon the amount of observations missing county data (<.5%) and increased the run time, so it will be left as commented out (also in the interest of reproducibility).
         # ,
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'elmendorfafb' &
         #                 school %in% c('Bartlett') &
         #                 state_province == 'AK',
         #               'Anchorage', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'bmgoldwaterafrange' &
         #                 school %in% c('Buckeye Union', 'Youngker', 'Verrado') &
         #                 state_province == 'AZ',
         #               'Buckeye', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'coronadontlforest' &
         #                 school %in% c('Cienega') &
         #                 state_province == 'AZ',
         #               'Vail', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'mtmeadowsarea' &
         #                 school %in% c('Lassen') &
         #                 state_province == 'CA',
         #               'Susanville', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'plumasntlforest' &
         #                 state_province == 'CA',
         #               'Quincy', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'westtehamacounty' &
         #                 school %in% c('Red Bluff') &
         #                 state_province == 'CA',
         #               'Red Bluff', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'pikentlforest' &
         #                 school %in% c('The Classical Academy') &
         #                 state_province == 'CO',
         #               'Colorado Springs', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'lakeminneola' &
         #                 school %in% c('Lake Minneola') &
         #                 state_province == 'FL',
         #               'Minneola', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'southwestbrevardcnty' &
         #                 grepl('rockledge', school, ignore.case = T) == T &
         #                 state_province == 'FL',
         #               'Rockledge', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'mcrae' &
         #                 school %in% c('Telfair County') &
         #                 state_province == 'GA',
         #               'McRae-Helena', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'hoffmanforest' &
         #                 school %in% c('Richlands') &
         #                 state_province == 'NC',
         #               'Richlands', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'ladysisland' &
         #                 school %in% c('Beaufort') &
         #                 state_province == 'SC',
         #               'Beaufort', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'alief' &
         #                 school %in% c('Taylor') &
         #                 state_province == 'TX',
         #               'Houston', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'brock' &
         #                 school %in% c('Brock') &
         #                 state_province == 'TX',
         #               'Weatherford', city),
         # city = ifelse(mgsub::mgsub(city,
         #                            pattern = c("[^[:alnum:] ]", " "),
         #                            replacement = '',
         #                            recycle = T) %>% tolower() == 'adna' &
         #                 school %in% c('Adna') &
         #                 state_province == 'WA',
         #               'Chehalis', city)
)
```

Construct universal city_updated column to make future joins easier
```{r}
#for recruit_player, uscities, and zip_cities data, make a column called city_updated with all non-alphanumeric characters removed and converted to lowercase. This will help in joining the data in the following steps.

recruiting_player <- recruiting_player %>% 
  mutate(city_updated = mgsub::mgsub(city, 
                                     pattern = c("[^[:alnum:] ]", " "), 
                                     replacement = '', recycle = T) %>% tolower())

uscities <- uscities %>% 
  mutate(city_updated = mgsub::mgsub(city_ascii, 
                                     pattern = c("[^[:alnum:] ]", " "), 
                                     replacement = '', recycle = T) %>% tolower())

zip_cities <- zip_cities %>% 
  mutate(city_updated = mgsub::mgsub(city, 
                                     pattern = c("[^[:alnum:] ]", " "), 
                                     replacement = '', recycle = T) %>% tolower())
```

## Combining Recruit and County Data

Join in data from `uscities`
```{r}
#join data and create separate dataframes depending on whether or not there is county data

#dataframe for observations with county data
recruiting_player_cty <- recruiting_player %>% 
  left_join(uscities[, c('city_updated', 'state_id', 'county_fips', 'county_name')],
            by = c('city_updated', 'state_province' = 'state_id')) %>% 
  filter(!is.na(county_name))

#dataframe for observations without county data
recruiting_player_n_cty <- recruiting_player %>% 
  left_join(uscities[, c('city_updated', 'state_id', 'county_fips', 'county_name')],
            by = c('city_updated', 'state_province' = 'state_id')) %>% 
  filter(is.na(county_name)) %>% 
  select(-county_fips, -county_name)

#assert that there are an equivalent amount of rows for the original recruiting_player dataframe and the sum of rows between both new dataframes with/without county data
assertable::assert_nrows(recruiting_player, nrow(recruiting_player_cty) + nrow(recruiting_player_n_cty))
```

Join in `zip_cities` to fill in missing county data
```{r}
#join data and create separate dataframes depending on whether or not there is county data

#dataframe for observations with county data
complete_obs <- recruiting_player_n_cty %>% 
  left_join(zip_cities[, c('city_updated', 'state_id', 'county_fips', 'county_name')],
            by = c('city_updated', 'state_province' = 'state_id')) %>% 
  filter(!is.na(county_name))

#dataframe for observations without county data
incomplete_obs <- recruiting_player_n_cty %>% 
  left_join(zip_cities[, c('city_updated', 'state_id', 'county_fips', 'county_name')],
            by = c('city_updated', 'state_province' = 'state_id')) %>% 
  filter(is.na(county_name)) %>% 
  select(-county_fips, -county_name)
```

Create final dataframe with complete data
```{r}
#create final dataframe for recruiting data merged with county data
final_df <- rbind(recruiting_player_cty, complete_obs)
```

Address incomplete data
```{r}
#from observations with no county, create dataframe for observations with a school listed
inc_school <- incomplete_obs %>% 
  filter(!is.na(school))

#from observations with no county, create dataframe for observations with no school listed
#initialize empty columns for county data (since it will not be joined in for these observations)
inc_no_school <- incomplete_obs %>% 
  filter(is.na(school)) %>% 
  mutate(county_fips = NA,
         county_name = NA)

#create a dataframe of school names which only appear once
#these will be used to fill in data for observations with a school, but no other location info listed
single_schools <- final_df %>% 
  #group completed data by school, county, county name
  group_by(school, county_fips, county_name) %>% 
  #find the count of each group (the number of observations from a school in a given county)
  summarize(count = n()) %>% 
  #ungroup
  ungroup() %>% 
  #group result by school
  group_by(school) %>% 
  #find count of each group (number of time a school appears)
  summarize(count = n()) %>% 
  #ungroup
  ungroup() %>% 
  #filter to display number of schools that only appear once
  filter(count == 1) %>% 
  #join in location information for these schools
  left_join(final_df %>% select(school, state_province, county_fips, county_name) %>% unique()) %>% 
  #drop count variable
  select(-count)

#create dataframe for schools which are only located within a single county
single_county_schools <- final_df %>% 
  #group by school, state, county 
  group_by(school, state_province, county_fips, county_name) %>% 
  #find the count of each group (number of observations from a school in a given state/county)
  summarize(count = n()) %>% 
  #ungroup
  ungroup() %>% 
  #group by school and state
  group_by(school, state_province) %>% 
  #find count of each group (number of times a school appears in a given state)
  summarize(count = n()) %>% 
  #ungroup
  ungroup() %>% 
  #display school/county groups which only appear once
  filter(count == 1) %>% 
  #join in location information for these schools
  left_join(final_df %>% select(school, state_province, county_fips, county_name) %>% unique()) %>%
  #drop count variable
  select(-count)

#create dataframe of incomplete data with schools listed and accompanying counties
added_schools <- inc_school %>% 
  #join single_schools dataframe
  left_join(single_schools, by = 'school') %>% 
  #join single_county_schools dataframe
  left_join(single_county_schools, by = c('school', 'state_province.x' = 'state_province')) %>% 
  #condense data from joins into single column
  mutate(state_province = ifelse(is.na(state_province.x), state_province.y, state_province.x),
         county_fips = ifelse(is.na(county_fips.x), county_fips.y, county_fips.x),
         county_name = ifelse(is.na(county_name.x), county_name.y, county_name.x)) %>% 
  #remove irrelevant columns
  select(-state_province.x, -state_province.y, 
         -county_fips.x, -county_fips.y, 
         -county_name.x, -county_name.y)  %>% 
  #reorder dataframe
  relocate(state_province, .before = country)
```

Complete final dataframe of recruit data and county data `final_df`
```{r}
#append existing final_df, added_schools, and inc_no_school
final_df <- rbind(final_df, added_schools, inc_no_school)

#assert that final_df has the same number of observations as the original recruit_player dataframe
assertable::assert_nrows(recruiting_player, nrow(final_df))
```

Display proportion pf missing county data in `final_df`
```{r}
#divide sum of NA fips codes by the total number of observations
sum(is.na(final_df$county_fips))/nrow(final_df)
```

Create column for position groups in `final_df`
```{r}
final_df$position_group <- NA
for(i in 1:nrow(final_df)) {
  if(final_df$position[i] %in% c('ILB', 'OLB')) {final_df$position_group[i] = 'LB'}
  if(final_df$position[i] %in% c('PRO', 'DUAL')) {final_df$position_group[i] = 'QB'}
  if(final_df$position[i] %in% c('WR')) {final_df$position_group[i] = 'WR'}
  if(final_df$position[i] %in% c('RB', 'APB', 'FB', 'SF')) {final_df$position_group[i] = 'RB'}
  if(final_df$position[i] %in% c('OT', 'OC', 'OG')) {final_df$position_group[i] = 'OL'}
  if(final_df$position[i] %in% c('CB', 'S')) {final_df$position_group[i] = 'DB'}
  if(final_df$position[i] %in% c('DT', 'SDE', 'WDE')) {final_df$position_group[i] = 'DL'}
  if(final_df$position[i] %in% c('TE')) {final_df$position_group[i] = 'TE'}
  if(final_df$position[i] %in% c('ATH')) {final_df$position_group[i] = 'ATH'}
  if(final_df$position[i] %in% c('K', 'P', 'LS', 'RET')) {final_df$position_group[i] = 'ST'}
}
```

Remove/Reorder columns of `final_df`
```{r}
final_df <- final_df %>% 
  select(-country,-city_updated) %>% 
  arrange(year, ranking)
```

## Add Location Data

Get locations of colleges
```{r}
# #select unique committed schools and join in school city data
# school_locs_final <- final_df %>% 
#   select(committed_to) %>% 
#   unique() %>% 
#   inner_join(school_locs, by = c('committed_to' = 'school')) %>% 
#   mutate(city_state = paste0(school_city, ', ', state_province))
# 
# #get latitude and longitude data for schools
# school_lat_lon <- mutate_geocode(school_locs_final, city_state)
# 
# #assert no NA values for latlon data and select/rename relevant columns
# school_lat_lon <- school_lat_lon %>% 
#   assert(not_na, lon:lat) %>% 
#   select(committed_to, lon, lat) %>% 
#   rename(school_lon = lon, school_lat = lat)
```

Get locations of recruits
```{r}
# #select unique recruit city states and get latlon data
# recruit_locs_final <- final_df %>% 
#   select(city, state_province) %>% 
#   unique() %>%
#   mutate(city_state = paste0(city, ', ', state_province)) %>% 
#   mutate_geocode(city_state)
# 
# #assert no NA values for latlon data and select/rename relevant columns
# recruit_lat_lon <- recruit_locs_final %>% 
#   assert(not_na, lon:lat) %>% 
#   select(-city_state) %>% 
#   rename(recruit_lon = lon, recruit_lat = lat)
```

Join distance data to `final_df` and calculate distance
```{r}
# final_df <- final_df %>% 
#   left_join(recruit_lat_lon, by = c('city', 'state_province')) %>% 
#   left_join(school_lat_lon, by = 'committed_to')
# 
# export <- final_df %>% 
#   #distHaversine calculates distance on a globe surface and divided by 1609 converts to mileage
#   mutate(dist_mi = purrr::pmap_dbl(., ~ distm(x = c(..17, ..18), y = c(..19, ..20),
#                                               fun = distHaversine) / 1609))
```

## Export Final Dataset

Write csv for complete recruiting data with county and distance data
```{r}
# write.csv(export, 'clean-data/recruiting_player.csv', row.names = F)
```

