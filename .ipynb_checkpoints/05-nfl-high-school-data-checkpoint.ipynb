{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-nfl-high-school-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data scraping for high schools of all NFL players (appeared in a game) since beginning of league"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook scrapes PFR data for all players and the high schools which they attended in the NFL dating back to 1920 (wow!). This data will be used for analysis comparing recruit counts and location density to NFL talent counts and location density. This code takes a few hours to run if all states are included. In the future, multiprocessing will be added as an ehancement to improve scraping time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to scrape each state for list of schools, input is list of state abbreviations\n",
    "def state_school(state_list):\n",
    "    \n",
    "    #initialize empty dataframe\n",
    "    all_school_df = pd.DataFrame(columns = ['hs_link', 'hs_name', 'hs_city', 'hs_state', 'num_players', 'num_active'])\n",
    "    \n",
    "    #iterate over each state in the list of states provided\n",
    "    for state in state_list:\n",
    "        \n",
    "        #set the relevant url, must do in parts then join together\n",
    "        url_parts = ['https://www.pro-football-reference.com/schools/high_schools.cgi?hs_state=', str(state)]\n",
    "        url = ''.join(url_parts)\n",
    "        #use get to access the url and save the page\n",
    "        page = rq.get(url)\n",
    "        #save the html content of the page\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "        \n",
    "        #initialize empty lists for scraped data to be stored\n",
    "        school_link = []\n",
    "        school_name = []\n",
    "        school_attributes = [[] for i in range(2)]\n",
    "\n",
    "        #grab the school data from the html content\n",
    "        for school in soup.find_all('th', class_ = 'left'):\n",
    "            school_link.append(school.find('a')['href'])\n",
    "            school_name.append(school.find('a').get_text())\n",
    "\n",
    "        #grab the school data from the html content\n",
    "        for school in soup.find_all('td'):\n",
    "            school_attributes[0].append(school.get('data-stat'))\n",
    "            school_attributes[1].append(school.get_text())\n",
    "\n",
    "        #initialize empty dataframe to store data\n",
    "        school_df = pd.DataFrame({\n",
    "            'hs_link': school_link,\n",
    "            'hs_name': school_name\n",
    "        })\n",
    "\n",
    "        #initialize empty dataframe to store data\n",
    "        attributes_df = pd.DataFrame({\n",
    "            'attribute': school_attributes[0],\n",
    "            'value': school_attributes[1]\n",
    "        })\n",
    "\n",
    "        #split attributes data into individual columns to be joined to full dataframe\n",
    "        hs_city = attributes_df[attributes_df.attribute == 'hs_city'].reset_index(drop = True).rename(columns={'value': 'hs_city'}).drop(['attribute'], axis = 1)\n",
    "        hs_state = attributes_df[attributes_df.attribute == 'hs_state'].reset_index(drop = True).rename(columns={'value': 'hs_state'}).drop(['attribute'], axis = 1)\n",
    "        num_players = attributes_df[attributes_df.attribute == 'num_players'].reset_index(drop = True).rename(columns={'value': 'num_players'}).drop(['attribute'], axis = 1)\n",
    "        num_active = attributes_df[attributes_df.attribute == 'num_active'].reset_index(drop = True).rename(columns={'value': 'num_active'}).drop(['attribute'], axis = 1)\n",
    "        \n",
    "        #concatenate all attributes for schools in the current state\n",
    "        state_school_df = pd.concat([school_df, hs_city, hs_state, num_players, num_active], axis = 1)\n",
    "        \n",
    "        #if there are null values, return this error message\n",
    "        if state_school_df.isnull().values.any() == True:\n",
    "            return('Null values detected in ' + str(state))\n",
    "\n",
    "        #append current state school results to all school list\n",
    "        all_school_df = all_school_df.append(state_school_df, ignore_index = True)\n",
    "    \n",
    "    #return finalized dataframe\n",
    "    return(all_school_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to scrape each school link for list of players, input is school dataframe from previous function\n",
    "def school_player(schools_df):\n",
    "    \n",
    "    #initialize empty dataframe\n",
    "    all_player_df = pd.DataFrame(columns = ['hs_link', 'player_link', 'player_name', 'pos', 'nfl_team', 'year_min', 'year_max']) \n",
    "    \n",
    "    #iterate over each school link in the list of schools provided\n",
    "    for hs in schools_df.hs_link:\n",
    "        \n",
    "        #set the relevant url, must do in parts then join together\n",
    "        url_parts = ['https://www.pro-football-reference.com/schools/', str(hs)]\n",
    "        url = ''.join(url_parts)\n",
    "        #use get to access the url and save the page\n",
    "        page = rq.get(url)\n",
    "        #save the html content of the page\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "\n",
    "        #initialize empty lists for scraped data to be stored\n",
    "        player_link = []\n",
    "        player_name = []\n",
    "        player_attributes = [[] for i in range(2)]\n",
    "\n",
    "        #grab the player data from the html content\n",
    "        for player in soup.find_all('th', scope = 'row'):\n",
    "            player_link.append(player.find('a')['href'])\n",
    "            player_name.append(player.find('a').get_text())\n",
    "\n",
    "        #grab the player data from the html content\n",
    "        for player in soup.find_all('td'):\n",
    "            player_attributes[0].append(player.get('data-stat'))\n",
    "            player_attributes[1].append(player.get_text())\n",
    "\n",
    "        #initialize empty dataframe to store data\n",
    "        player_df = pd.DataFrame({\n",
    "            'player_link': player_link,\n",
    "            'player_name': player_name\n",
    "        })\n",
    "\n",
    "        #initialize empty dataframe to store data\n",
    "        attributes_df = pd.DataFrame({\n",
    "            'attribute': player_attributes[0],\n",
    "            'value': player_attributes[1]\n",
    "        })\n",
    "\n",
    "        #split attributes data into individual columns to be joined to full dataframe\n",
    "        position = attributes_df[attributes_df.attribute.astype(str) == 'pos'].reset_index(drop = True).rename(columns={'value': 'pos'}).drop(['attribute'], axis = 1)\n",
    "        nfl_team = attributes_df[attributes_df.attribute.astype(str) == 'teams'].reset_index(drop = True).rename(columns={'value': 'nfl_team'}).drop(['attribute'], axis = 1)\n",
    "        year_min = attributes_df[attributes_df.attribute.astype(str) == 'year_min'].reset_index(drop = True).rename(columns={'value': 'year_min'}).drop(['attribute'], axis = 1)\n",
    "        year_max = attributes_df[attributes_df.attribute.astype(str) == 'year_max'].reset_index(drop = True).rename(columns={'value': 'year_max'}).drop(['attribute'], axis = 1)\n",
    "\n",
    "        #concatenate all attributes for players in the current school\n",
    "        school_player_df = pd.concat([player_df, position, nfl_team, year_min, year_max], axis = 1)\n",
    "\n",
    "        #if there are null values, return this error message\n",
    "        if school_player_df.isnull().values.any() == True:\n",
    "            return('Null values detected in ' + str(hs))\n",
    "\n",
    "        #repeat the link of the current school for the length of the school_player_df\n",
    "        hs_link_df = pd.DataFrame(np.repeat(hs, [len(school_player_df)]), columns = ['hs_link'])\n",
    "\n",
    "        #concatenate links and player info, append to large dataframe\n",
    "        all_player_df = all_player_df.append(pd.concat([hs_link_df, school_player_df], axis = 1), ignore_index = True)\n",
    "        \n",
    "    #remove duplicated player values (result of one school link being listed twice for separate schools)\n",
    "    all_player_df = all_player_df[~all_player_df.duplicated()]\n",
    "    \n",
    "    #merge player and school data (inner join)\n",
    "    export = schools_df.merge(all_player_df, on = 'hs_link')\n",
    "    \n",
    "    #remove observations where the school link is the incorrect one, reset index\n",
    "    #drop num_players and num_active because they are often inaccurate\n",
    "    export = export[~((export.hs_link == 'high_schools.cgi?id=93bdb950') & (export.hs_state == 'MI'))].reset_index(drop = True).drop(['num_players', 'num_active'], axis = 1)\n",
    "\n",
    "    #return finalized dataframe    \n",
    "    return(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to remove duplicate players (players which attended multiple hs), input is school player dataframe from previous function\n",
    "def hs_player_cleaning(hs_player_df):\n",
    "    \n",
    "    #get counts of the amount of times that each player appears, filter to only include those who appear more than once\n",
    "    duplicated_players = hs_player_df.groupby(['player_link']).filter(lambda x: x['hs_link'].count() > 1)\n",
    "    #create column that indicates these are duplicated players\n",
    "    duplicated_players.insert(9, 'duplicated', True)\n",
    "    \n",
    "    #merge duplicated players back into high school player dataframe (shows which players were duplicated)\n",
    "    merged_players = hs_player_df.merge(duplicated_players, how = 'left')\n",
    "\n",
    "    #check to ensure that the amount of rows in the input dataframe matches the amount of rows in the previously joined dataframe\n",
    "    if len(merged_players) != len(hs_player_df):\n",
    "        return('Input and merge tables not of equal length.')\n",
    "\n",
    "    #separate the players with single and duplicate observations\n",
    "    single_players = merged_players[merged_players['duplicated'].isnull()].reset_index(drop = True).drop(['duplicated'], axis = 1)\n",
    "    duplicated_players = duplicated_players.reset_index(drop = True).drop(['duplicated'], axis = 1)\n",
    "\n",
    "    #check to ensure that the sum of rows in the newly split dataframes matches the amount of rows in the previously joined dataframe\n",
    "    if len(merged_players) != len(single_players) + len(duplicated_players):\n",
    "        return('Merged tables and single + duplicate tables not of equal length.')\n",
    "\n",
    "    #initialize empty lists for scraped data to be stored\n",
    "    hs_list = []\n",
    "    player_list = []\n",
    "\n",
    "    #iterate over each diplicated player, we are interested in the last high school that each attended\n",
    "    for p in duplicated_players.player_link.unique():\n",
    "\n",
    "        #initialize empty list to store data for each hs that current player attended\n",
    "        player_hs_list = []\n",
    "\n",
    "        #add current player to list of players\n",
    "        player_list.append(p)\n",
    "\n",
    "        #accessing webpage\n",
    "        #set the relevant url, must do in parts then join together\n",
    "        url_parts = ['https://www.pro-football-reference.com', str(p)]\n",
    "        url = ''.join(url_parts)\n",
    "        #use get to access the url and save the page\n",
    "        page = rq.get(url)\n",
    "        #save the html content of the page\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "\n",
    "        #grab player data from html content\n",
    "        for attribute in soup.find_all('a'):\n",
    "            if attribute.has_attr('href'):\n",
    "                if attribute['href'] in '/schools/' + merged_players.hs_link.unique():\n",
    "                    player_hs_list.append(attribute['href'])\n",
    "\n",
    "        #append the last high school to the large high school list\n",
    "        hs_list.append(player_hs_list[-1])\n",
    "\n",
    "    #create a dataframe from the player and high school data\n",
    "    duplicate_last_school = pd.DataFrame({\n",
    "        'hs_link': hs_list,\n",
    "        'player_link': player_list\n",
    "    })\n",
    "\n",
    "    #clean the high school links to maintain data consistency\n",
    "    duplicate_last_school['hs_link'] = duplicate_last_school['hs_link'].str.replace('/schools/', '')\n",
    "\n",
    "    #inner join to keep only the last school observations for each duplicated player\n",
    "    duplicates_cleaned = duplicated_players.merge(duplicate_last_school, how = 'inner')\n",
    "\n",
    "    #combine single and cleaned duplicated dataframes into dataset to be exported\n",
    "    export = pd.concat([single_players, duplicates_cleaned], axis = 0).reset_index(drop = True)\n",
    "\n",
    "    #return the export dataset\n",
    "    return(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#states list to be used in function\n",
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run state_school with specified list of states and save\n",
    "all_schools = state_school(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run school_player function on returned dataframe from above cell and save\n",
    "all_players = school_player(all_schools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run hs_player_cleaning function on returned dataframe from above cell and save\n",
    "export = hs_player_cleaning(all_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "This is now taken care of in an above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicated player values (result of one school link being listed twice for separate schools)\n",
    "#all_players = all_players[~all_players.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge player and school data (inner join)\n",
    "#export = all_schools.merge(all_players, on = 'hs_link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove observations where the school link is the incorrect one, reset index\n",
    "#drop num_players and num_active because they are often inaccurate\n",
    "#export = export[~((export.hs_link == 'high_schools.cgi?id=93bdb950') & (export.hs_state == 'MI'))].reset_index(drop = True).drop(['num_players', 'num_active'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hs_link</th>\n",
       "      <th>hs_name</th>\n",
       "      <th>hs_city</th>\n",
       "      <th>hs_state</th>\n",
       "      <th>player_link</th>\n",
       "      <th>player_name</th>\n",
       "      <th>pos</th>\n",
       "      <th>nfl_team</th>\n",
       "      <th>year_min</th>\n",
       "      <th>year_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_schools.cgi?id=93b98891</td>\n",
       "      <td>Woodlawn</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>AL</td>\n",
       "      <td>/players/M/McKiCa01.htm</td>\n",
       "      <td>Cassanova McKinzy</td>\n",
       "      <td>LB</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2018</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high_schools.cgi?id=93b98891</td>\n",
       "      <td>Woodlawn</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>AL</td>\n",
       "      <td>/players/D/DansKa20.htm</td>\n",
       "      <td>Karlos Dansby</td>\n",
       "      <td>LB</td>\n",
       "      <td>ARI,MIA,CLE,CIN</td>\n",
       "      <td>2004</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high_schools.cgi?id=93b98891</td>\n",
       "      <td>Woodlawn</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>AL</td>\n",
       "      <td>/players/D/DaviCh05.htm</td>\n",
       "      <td>Chris Davis</td>\n",
       "      <td>CB</td>\n",
       "      <td>SDG,SFO</td>\n",
       "      <td>2014</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high_schools.cgi?id=93b98891</td>\n",
       "      <td>Woodlawn</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>AL</td>\n",
       "      <td>/players/C/CarrGr20.htm</td>\n",
       "      <td>Gregg Carr</td>\n",
       "      <td>LB</td>\n",
       "      <td>PIT</td>\n",
       "      <td>1985</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high_schools.cgi?id=93b98891</td>\n",
       "      <td>Woodlawn</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>AL</td>\n",
       "      <td>/players/N/NathTo00.htm</td>\n",
       "      <td>Tony Nathan</td>\n",
       "      <td>RB</td>\n",
       "      <td>MIA</td>\n",
       "      <td>1979</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25008</th>\n",
       "      <td>high_schools.cgi?id=93b902b7</td>\n",
       "      <td>Keewatin Academy</td>\n",
       "      <td>Prairie du Chien</td>\n",
       "      <td>WI</td>\n",
       "      <td>/players/B/BarrJo21.htm</td>\n",
       "      <td>Johnny Barrett</td>\n",
       "      <td>E-HB</td>\n",
       "      <td>CHT</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009</th>\n",
       "      <td>high_schools.cgi?id=93bbc56a</td>\n",
       "      <td>University</td>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>WI</td>\n",
       "      <td>/players/D/DreyWa20.htm</td>\n",
       "      <td>Wally Dreyer</td>\n",
       "      <td>DB-HB</td>\n",
       "      <td>CHI,GNB</td>\n",
       "      <td>1949</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25010</th>\n",
       "      <td>high_schools.cgi?id=93bd3ce8</td>\n",
       "      <td>South Milwaukee</td>\n",
       "      <td>South Milwaukee</td>\n",
       "      <td>WI</td>\n",
       "      <td>/players/H/HugaHa20.htm</td>\n",
       "      <td>Harry Hugasian</td>\n",
       "      <td>HB-DB</td>\n",
       "      <td>CHI,BAL</td>\n",
       "      <td>1955</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25011</th>\n",
       "      <td>high_schools.cgi?id=93c0f594</td>\n",
       "      <td>St. John Academy</td>\n",
       "      <td>Green Bay</td>\n",
       "      <td>WI</td>\n",
       "      <td>/players/W/WoodWh20.htm</td>\n",
       "      <td>Whitey Woodin</td>\n",
       "      <td>G-T</td>\n",
       "      <td>GNB,RAC</td>\n",
       "      <td>1922</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25012</th>\n",
       "      <td>high_schools.cgi?id=93befbf3</td>\n",
       "      <td>Elderton</td>\n",
       "      <td>Elderton</td>\n",
       "      <td>WI</td>\n",
       "      <td>/players/M/MurrDo20.htm</td>\n",
       "      <td>Don Murry</td>\n",
       "      <td>T-E-G</td>\n",
       "      <td>RAC,CHI</td>\n",
       "      <td>1922</td>\n",
       "      <td>1932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25013 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hs_link           hs_name           hs_city  \\\n",
       "0      high_schools.cgi?id=93b98891          Woodlawn        Birmingham   \n",
       "1      high_schools.cgi?id=93b98891          Woodlawn        Birmingham   \n",
       "2      high_schools.cgi?id=93b98891          Woodlawn        Birmingham   \n",
       "3      high_schools.cgi?id=93b98891          Woodlawn        Birmingham   \n",
       "4      high_schools.cgi?id=93b98891          Woodlawn        Birmingham   \n",
       "...                             ...               ...               ...   \n",
       "25008  high_schools.cgi?id=93b902b7  Keewatin Academy  Prairie du Chien   \n",
       "25009  high_schools.cgi?id=93bbc56a        University         Milwaukee   \n",
       "25010  high_schools.cgi?id=93bd3ce8   South Milwaukee   South Milwaukee   \n",
       "25011  high_schools.cgi?id=93c0f594  St. John Academy         Green Bay   \n",
       "25012  high_schools.cgi?id=93befbf3          Elderton          Elderton   \n",
       "\n",
       "      hs_state              player_link        player_name    pos  \\\n",
       "0           AL  /players/M/McKiCa01.htm  Cassanova McKinzy     LB   \n",
       "1           AL  /players/D/DansKa20.htm      Karlos Dansby     LB   \n",
       "2           AL  /players/D/DaviCh05.htm       Chris Davis      CB   \n",
       "3           AL  /players/C/CarrGr20.htm         Gregg Carr     LB   \n",
       "4           AL  /players/N/NathTo00.htm        Tony Nathan     RB   \n",
       "...        ...                      ...                ...    ...   \n",
       "25008       WI  /players/B/BarrJo21.htm     Johnny Barrett   E-HB   \n",
       "25009       WI  /players/D/DreyWa20.htm       Wally Dreyer  DB-HB   \n",
       "25010       WI  /players/H/HugaHa20.htm     Harry Hugasian  HB-DB   \n",
       "25011       WI  /players/W/WoodWh20.htm      Whitey Woodin    G-T   \n",
       "25012       WI  /players/M/MurrDo20.htm          Don Murry  T-E-G   \n",
       "\n",
       "              nfl_team  year_min  year_max  \n",
       "0                  WAS      2018      2019  \n",
       "1      ARI,MIA,CLE,CIN      2004      2017  \n",
       "2              SDG,SFO      2014      2016  \n",
       "3                  PIT      1985      1988  \n",
       "4                  MIA      1979      1987  \n",
       "...                ...       ...       ...  \n",
       "25008              CHT      1920      1920  \n",
       "25009          CHI,GNB      1949      1950  \n",
       "25010          CHI,BAL      1955      1955  \n",
       "25011          GNB,RAC      1922      1931  \n",
       "25012          RAC,CHI      1922      1932  \n",
       "\n",
       "[25013 rows x 10 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data to csv\n",
    "export.to_csv('preliminary-data/nfl-player-high-schools.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
